# -*- coding: utf-8 -*-
"""ReTrain1.0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K9mK39buYtEPLfxOTLDI2uqmq9ok4wMk
"""

import pandas as pd

# Load raw dataset
df = pd.read_csv('/content/label_data.csv')

# Remove duplicates (based on combined headline + bio)
df['combined'] = df['headline'].fillna('') + ' ' + df['bio'].fillna('')
df = df.drop_duplicates(subset=['combined'])
df = df.drop(columns=['combined'])

# Drop rows where headline or bio is missing
df = df.dropna(subset=['headline', 'bio'])

# Strip whitespace and lowercase
df['headline'] = df['headline'].str.strip().str.lower()
df['bio'] = df['bio'].str.strip().str.lower()

# Filter labels: only 'real' or 'fake'
df = df[df['label'].isin(['real', 'fake'])]

# Reset index
df = df.reset_index(drop=True)

# Save cleaned dataset
df.to_csv('cleaned_dataset.csv', index=False)

print(f"✅ Cleaned dataset saved: {df.shape[0]} samples")

pip install textstat

import pandas as pd
import numpy as np
import json
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from textstat import flesch_reading_ease
import joblib
import os

# Load cleaned dataset
df = pd.read_csv("/content/cleaned_dataset.csv")

# Load buzzword list
with open("/content/buzzwordlist.json") as f:
    buzzwords = set(json.load(f)["buzzwords"])

# --- Feature Engineering Functions ---

def count_buzzwords(text):
    words = re.findall(r'\b\w+\b', text.lower())
    return sum(1 for word in words if word in buzzwords)

def buzzword_density(text):
    words = re.findall(r'\b\w+\b', text.lower())
    total_words = len(words)
    matches = count_buzzwords(text)
    return matches / total_words if total_words > 0 else 0

def readability(text):
    try:
        score = flesch_reading_ease(text)
        return score if not np.isnan(score) else 0
    except:
        return 0

def extract_lexical_features(df):
    features = pd.DataFrame()
    for field in ['headline', 'bio']:
        features[f"{field}_char_count"] = df[field].str.len()
        features[f"{field}_word_count"] = df[field].apply(lambda x: len(x.split()))
        features[f"{field}_readability"] = df[field].apply(readability)
        features[f"{field}_buzzword_matches"] = df[field].apply(count_buzzwords)
        features[f"{field}_buzzword_density"] = df[field].apply(buzzword_density)
    return features

# --- TF-IDF Vectorizers ---

headline_vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(3, 5), max_features=100)

bio_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=100)

X_tfidf_headline = headline_vectorizer.fit_transform(df['headline'])
X_tfidf_bio = bio_vectorizer.fit_transform(df['bio'])

# --- Combine Features ---
X_lexical = extract_lexical_features(df)
X_combined = np.hstack([X_lexical.values, X_tfidf_headline.toarray(), X_tfidf_bio.toarray()])

# Labels
y = df['label'].map({'real': 1, 'fake': 0}).values

# --- Save Vectorizers ---
os.makedirs('vectorizer', exist_ok=True)
joblib.dump(headline_vectorizer, 'vectorizer/headline_vectorizer.pkl')
joblib.dump(bio_vectorizer, 'vectorizer/bio_vectorizer.pkl')

# --- Save Feature Columns ---
feature_columns = list(X_lexical.columns) + \
                  [f"headline_tfidf_{i}" for i in range(X_tfidf_headline.shape[1])] + \
                  [f"bio_tfidf_{i}" for i in range(X_tfidf_bio.shape[1])]

with open("vectorizer/feature_columns.json", "w") as f:
    json.dump(feature_columns, f, indent=2)

# --- Save full feature matrix and labels (optional for inspection) ---
np.save('vectorizer/X_combined.npy', X_combined)
np.save('vectorizer/y.npy', y)

print(f"✅ Feature extraction completed. Feature matrix shape: {X_combined.shape}")
print(f"Features saved to vectorizer/feature_columns.json")

import numpy as np
import pandas as pd
import json
import joblib
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, roc_auc_score
import os

# Load feature matrix and labels
X = np.load('vectorizer/X_combined.npy')
y = np.load('vectorizer/y.npy')

# Split data into train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)


# Train Logistic Regression
model = LogisticRegression(max_iter=2000)
model.fit(X_train, y_train)

# Evaluate
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]

print("✅ Classification Report:")
print(classification_report(y_test, y_pred))

roc_score = roc_auc_score(y_test, y_prob)
print(f"✅ ROC AUC Score: {roc_score:.3f}")

# Save model
os.makedirs("models", exist_ok=True)
joblib.dump(model, "models/logistic_model.pkl")

# ✅ Load existing config, update only model file/version if needed
config_path = "/content/config.json"
with open(config_path) as f:
    config = json.load(f)

config["model"] = "models/logistic_model.pkl"
config["model_version"] = "1.0.0"
config["vectorizers"]["headline_vectorizer"] = "vectorizer/headline_vectorizer.pkl"
config["vectorizers"]["bio_vectorizer"] = "vectorizer/bio_vectorizer.pkl"
config["vectorizers"]["feature_columns"] = "vectorizer/feature_columns.json"


with open(config_path, "w") as f:
    json.dump(config, f, indent=2)

print("✅ Model saved to models/logistic_model.pkl")
print(f"✅ Config UPDATED at {config_path} — regex, buzzword_file, and other settings PRESERVED")

import joblib
import numpy as np
import json
import re
import os
from sklearn.feature_extraction.text import TfidfVectorizer
import textstat # Import textstat here

CONFIG_PATH = "/content/config.json"
BUZZWORD_PATH = "/content/buzzwordlist.json"

# ---- Load Config ----
with open(CONFIG_PATH) as f:
    config = json.load(f)

with open(BUZZWORD_PATH) as f:
    buzzwords = set(json.load(f)["buzzwords"])

regex_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in config["regex_blacklist_patterns"]]
reason_templates = config.get("reason_templates", {})

# ---- Load Model & Vectorizers ----
model = joblib.load(os.path.join("/content/", config["model"]))
headline_vectorizer = joblib.load(os.path.join("/content/", config["vectorizers"]["headline_vectorizer"]))
bio_vectorizer = joblib.load(os.path.join("/content/", config["vectorizers"]["bio_vectorizer"]))

with open(os.path.join("/content/", config["vectorizers"]["feature_columns"])) as f:
    feature_columns = json.load(f)

# ---- Feature Extraction Functions ----
def count_buzzwords(text):
    words = re.findall(r'\b\w+\b', text.lower())
    return sum(1 for word in words if word in buzzwords)

def buzzword_density(text):
    words = re.findall(r'\b\w+\b', text.lower())
    total_words = len(words)
    matches = count_buzzwords(text)
    return matches / total_words if total_words > 0 else 0

def readability(text):
    try:
        # Use the imported textstat
        score = textstat.flesch_reading_ease(text)
        return score if score == score else 0  # filter NaN
    except:
        return 0

def extract_features(profile_data):
    headline = profile_data.get("headline", "").strip().lower()
    bio = profile_data.get("bio", "").strip().lower()

    features = {
        "headline_char_count": len(headline),
        "headline_word_count": len(headline.split()),
        "headline_readability": readability(headline),
        "headline_buzzword_matches": count_buzzwords(headline),
        "headline_buzzword_density": buzzword_density(headline),
        "bio_char_count": len(bio),
        "bio_word_count": len(bio.split()),
        "bio_readability": readability(bio),
        "bio_buzzword_matches": count_buzzwords(bio),
        "bio_buzzword_density": buzzword_density(bio)
    }

    tfidf_headline = headline_vectorizer.transform([headline]).toarray().flatten()
    tfidf_bio = bio_vectorizer.transform([bio]).toarray().flatten()

    feature_vector = np.concatenate([list(features.values()), tfidf_headline, tfidf_bio])

    return feature_vector, features, headline, bio

# ---- Generate Verdict ----
def predict_profile(user_id, profile_data):
    feature_vector, features, headline, bio = extract_features(profile_data)

    proba = model.predict_proba([feature_vector])[0][1]  # Probability of class 1 = real/authentic
    authenticity_score = float(f"{proba:.3f}")

    thresholds = config["thresholds"]

    if authenticity_score < thresholds["likely_fabricated"]:
        verdict = "likely_fabricated"
    elif authenticity_score < thresholds["borderline"]:
        verdict = "borderline"
    else:
        verdict = "authentic"

    # Reasons and flagged fields
    flagged_fields = []
    reasons = []

    # Regex pattern matching
    for field_name, text in zip(["headline", "bio"], [headline, bio]):
        for pattern in regex_patterns:
            if pattern.search(text):
                flagged_fields.append(field_name)
                reasons.append(reason_templates.get("regex_match", "Contains overused phrases"))
                break

    # Buzzword density
    for field_name in ["headline", "bio"]:
        density = features[f"{field_name}_buzzword_density"]
        if density > 0.1:  # Example threshold
            if field_name not in flagged_fields:
                flagged_fields.append(field_name)
            reasons.append(reason_templates.get("buzzword_density", "Overused buzzwords detected"))

    # Low readability
    for field_name in ["headline", "bio"]:
        if features[f"{field_name}_readability"] < 30:
            if field_name not in flagged_fields:
                flagged_fields.append(field_name)
            reasons.append(reason_templates.get("low_readability", "Low readability suggests fabricated content"))

    return {
        "user_id": user_id,
        "authenticity_score": authenticity_score,
        "verdict": verdict,
        "reason": "; ".join(set(reasons)) if reasons else "No strong indicators of fabrication",
        "flagged_fields": flagged_fields
    }

# ---- TEST CASE ----
if __name__ == "__main__":
    sample_input = {
  "user_id": "stu_test_002",
  "profile_data": {
    "headline": "Visionary thought leader driving digital transformation",
    "bio": "A results-driven professional with a proven track record in delivering business value through strategic execution, stakeholder engagement, and transformative innovation in agile environments."
  }
}


    result = predict_profile(sample_input["user_id"], sample_input["profile_data"])
    print(json.dumps(result, indent=2))

